{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T07:05:24.636528Z",
     "start_time": "2018-09-22T07:05:18.681415Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:32:12.593580Z",
     "start_time": "2018-09-21T19:32:09.420100Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:21:56.422066Z",
     "start_time": "2018-09-21T19:21:56.418102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:26:48.251852Z",
     "start_time": "2018-09-21T19:26:48.247830Z"
    }
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "#gpus = [0]\n",
    "#torch.cuda.set_device(gpus[0])\n",
    "# if you want to train model with GPU, implement this code\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:27:33.191657Z",
     "start_time": "2018-09-21T19:27:33.184648Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data): #dataloader를 써라 ~\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    \n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch #yield는 매번 뱉는다\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:57:38.158879Z",
     "start_time": "2018-09-21T19:57:38.154895Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:31:25.213865Z",
     "start_time": "2018-09-21T19:31:25.196912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:35:03.534818Z",
     "start_time": "2018-09-21T19:35:02.377346Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'],\n",
       " ['ETYMOLOGY', '.'],\n",
       " ['(',\n",
       "  'Supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'Late',\n",
       "  'Consumptive',\n",
       "  'Usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'Grammar',\n",
       "  'School',\n",
       "  ')'],\n",
       " ['The',\n",
       "  'pale',\n",
       "  'Usher',\n",
       "  '--',\n",
       "  'threadbare',\n",
       "  'in',\n",
       "  'coat',\n",
       "  ',',\n",
       "  'heart',\n",
       "  ',',\n",
       "  'body',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brain',\n",
       "  ';',\n",
       "  'I',\n",
       "  'see',\n",
       "  'him',\n",
       "  'now',\n",
       "  '.'],\n",
       " ['He',\n",
       "  'was',\n",
       "  'ever',\n",
       "  'dusting',\n",
       "  'his',\n",
       "  'old',\n",
       "  'lexicons',\n",
       "  'and',\n",
       "  'grammars',\n",
       "  ',',\n",
       "  'with',\n",
       "  'a',\n",
       "  'queer',\n",
       "  'handkerchief',\n",
       "  ',',\n",
       "  'mockingly',\n",
       "  'embellished',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'gay',\n",
       "  'flags',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'known',\n",
       "  'nations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  '.'],\n",
       " ['He',\n",
       "  'loved',\n",
       "  'to',\n",
       "  'dust',\n",
       "  'his',\n",
       "  'old',\n",
       "  'grammars',\n",
       "  ';',\n",
       "  'it',\n",
       "  'somehow',\n",
       "  'mildly',\n",
       "  'reminded',\n",
       "  'him',\n",
       "  'of',\n",
       "  'his',\n",
       "  'mortality',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'While',\n",
       "  'you',\n",
       "  'take',\n",
       "  'in',\n",
       "  'hand',\n",
       "  'to',\n",
       "  'school',\n",
       "  'others',\n",
       "  ',',\n",
       "  'and',\n",
       "  'to',\n",
       "  'teach',\n",
       "  'them',\n",
       "  'by',\n",
       "  'what',\n",
       "  'name',\n",
       "  'a',\n",
       "  'whale',\n",
       "  '-',\n",
       "  'fish',\n",
       "  'is',\n",
       "  'to',\n",
       "  'be',\n",
       "  'called',\n",
       "  'in',\n",
       "  'our',\n",
       "  'tongue',\n",
       "  'leaving',\n",
       "  'out',\n",
       "  ',',\n",
       "  'through',\n",
       "  'ignorance',\n",
       "  ',',\n",
       "  'the',\n",
       "  'letter',\n",
       "  'H',\n",
       "  ',',\n",
       "  'which',\n",
       "  'almost',\n",
       "  'alone',\n",
       "  'maketh',\n",
       "  'the',\n",
       "  'signification',\n",
       "  'of',\n",
       "  'the',\n",
       "  'word',\n",
       "  ',',\n",
       "  'you',\n",
       "  'deliver',\n",
       "  'that',\n",
       "  'which',\n",
       "  'is',\n",
       "  'not',\n",
       "  'true',\n",
       "  '.\"'],\n",
       " ['--', 'HACKLUYT'],\n",
       " ['\"', 'WHALE', '.'],\n",
       " ['...', 'Sw', '.', 'and', 'Dan', '.'],\n",
       " ['HVAL', '.'],\n",
       " ['This',\n",
       "  'animal',\n",
       "  'is',\n",
       "  'named',\n",
       "  'from',\n",
       "  'roundness',\n",
       "  'or',\n",
       "  'rolling',\n",
       "  ';',\n",
       "  'for',\n",
       "  'in',\n",
       "  'Dan',\n",
       "  '.'],\n",
       " ['HVALT', 'is', 'arched', 'or', 'vaulted', '.\"'],\n",
       " ['--', 'WEBSTER', \"'\", 'S', 'DICTIONARY'],\n",
       " ['\"', 'WHALE', '.'],\n",
       " ['...'],\n",
       " ['It', 'is', 'more', 'immediately', 'from', 'the', 'Dut', '.'],\n",
       " ['and', 'Ger', '.'],\n",
       " ['WALLEN',\n",
       "  ';',\n",
       "  'A',\n",
       "  '.',\n",
       "  'S',\n",
       "  '.',\n",
       "  'WALW',\n",
       "  '-',\n",
       "  'IAN',\n",
       "  ',',\n",
       "  'to',\n",
       "  'roll',\n",
       "  ',',\n",
       "  'to',\n",
       "  'wallow',\n",
       "  '.\"'],\n",
       " ['--', 'RICHARDSON', \"'\", 'S', 'DICTIONARY'],\n",
       " ['KETOS', ',', 'GREEK', '.'],\n",
       " ['CETUS', ',', 'LATIN', '.'],\n",
       " ['WHOEL', ',', 'ANGLO', '-', 'SAXON', '.'],\n",
       " ['HVALT', ',', 'DANISH', '.'],\n",
       " ['WAL', ',', 'DUTCH', '.'],\n",
       " ['HWAL', ',', 'SWEDISH', '.'],\n",
       " ['WHALE', ',', 'ICELANDIC', '.'],\n",
       " ['WHALE', ',', 'ENGLISH', '.'],\n",
       " ['BALEINE', ',', 'FRENCH', '.'],\n",
       " ['BALLENA', ',', 'SPANISH', '.'],\n",
       " ['PEKEE', '-', 'NUEE', '-', 'NUEE', ',', 'FEGEE', '.'],\n",
       " ['PEKEE', '-', 'NUEE', '-', 'NUEE', ',', 'ERROMANGOAN', '.'],\n",
       " ['EXTRACTS',\n",
       "  '(',\n",
       "  'Supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'Sub',\n",
       "  '-',\n",
       "  'Sub',\n",
       "  '-',\n",
       "  'Librarian',\n",
       "  ').'],\n",
       " ['It',\n",
       "  'will',\n",
       "  'be',\n",
       "  'seen',\n",
       "  'that',\n",
       "  'this',\n",
       "  'mere',\n",
       "  'painstaking',\n",
       "  'burrower',\n",
       "  'and',\n",
       "  'grub',\n",
       "  '-',\n",
       "  'worm',\n",
       "  'of',\n",
       "  'a',\n",
       "  'poor',\n",
       "  'devil',\n",
       "  'of',\n",
       "  'a',\n",
       "  'Sub',\n",
       "  '-',\n",
       "  'Sub',\n",
       "  'appears',\n",
       "  'to',\n",
       "  'have',\n",
       "  'gone',\n",
       "  'through',\n",
       "  'the',\n",
       "  'long',\n",
       "  'Vaticans',\n",
       "  'and',\n",
       "  'street',\n",
       "  '-',\n",
       "  'stalls',\n",
       "  'of',\n",
       "  'the',\n",
       "  'earth',\n",
       "  ',',\n",
       "  'picking',\n",
       "  'up',\n",
       "  'whatever',\n",
       "  'random',\n",
       "  'allusions',\n",
       "  'to',\n",
       "  'whales',\n",
       "  'he',\n",
       "  'could',\n",
       "  'anyways',\n",
       "  'find',\n",
       "  'in',\n",
       "  'any',\n",
       "  'book',\n",
       "  'whatsoever',\n",
       "  ',',\n",
       "  'sacred',\n",
       "  'or',\n",
       "  'profane',\n",
       "  '.'],\n",
       " ['Therefore',\n",
       "  'you',\n",
       "  'must',\n",
       "  'not',\n",
       "  ',',\n",
       "  'in',\n",
       "  'every',\n",
       "  'case',\n",
       "  'at',\n",
       "  'least',\n",
       "  ',',\n",
       "  'take',\n",
       "  'the',\n",
       "  'higgledy',\n",
       "  '-',\n",
       "  'piggledy',\n",
       "  'whale',\n",
       "  'statements',\n",
       "  ',',\n",
       "  'however',\n",
       "  'authentic',\n",
       "  ',',\n",
       "  'in',\n",
       "  'these',\n",
       "  'extracts',\n",
       "  ',',\n",
       "  'for',\n",
       "  'veritable',\n",
       "  'gospel',\n",
       "  'cetology',\n",
       "  '.'],\n",
       " ['Far', 'from', 'it', '.'],\n",
       " ['As',\n",
       "  'touching',\n",
       "  'the',\n",
       "  'ancient',\n",
       "  'authors',\n",
       "  'generally',\n",
       "  ',',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'the',\n",
       "  'poets',\n",
       "  'here',\n",
       "  'appearing',\n",
       "  ',',\n",
       "  'these',\n",
       "  'extracts',\n",
       "  'are',\n",
       "  'solely',\n",
       "  'valuable',\n",
       "  'or',\n",
       "  'entertaining',\n",
       "  ',',\n",
       "  'as',\n",
       "  'affording',\n",
       "  'a',\n",
       "  'glancing',\n",
       "  'bird',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'eye',\n",
       "  'view',\n",
       "  'of',\n",
       "  'what',\n",
       "  'has',\n",
       "  'been',\n",
       "  'promiscuously',\n",
       "  'said',\n",
       "  ',',\n",
       "  'thought',\n",
       "  ',',\n",
       "  'fancied',\n",
       "  ',',\n",
       "  'and',\n",
       "  'sung',\n",
       "  'of',\n",
       "  'Leviathan',\n",
       "  ',',\n",
       "  'by',\n",
       "  'many',\n",
       "  'nations',\n",
       "  'and',\n",
       "  'generations',\n",
       "  ',',\n",
       "  'including',\n",
       "  'our',\n",
       "  'own',\n",
       "  '.'],\n",
       " ['So',\n",
       "  'fare',\n",
       "  'thee',\n",
       "  'well',\n",
       "  ',',\n",
       "  'poor',\n",
       "  'devil',\n",
       "  'of',\n",
       "  'a',\n",
       "  'Sub',\n",
       "  '-',\n",
       "  'Sub',\n",
       "  ',',\n",
       "  'whose',\n",
       "  'commentator',\n",
       "  'I',\n",
       "  'am',\n",
       "  '.'],\n",
       " ['Thou',\n",
       "  'belongest',\n",
       "  'to',\n",
       "  'that',\n",
       "  'hopeless',\n",
       "  ',',\n",
       "  'sallow',\n",
       "  'tribe',\n",
       "  'which',\n",
       "  'no',\n",
       "  'wine',\n",
       "  'of',\n",
       "  'this',\n",
       "  'world',\n",
       "  'will',\n",
       "  'ever',\n",
       "  'warm',\n",
       "  ';',\n",
       "  'and',\n",
       "  'for',\n",
       "  'whom',\n",
       "  'even',\n",
       "  'Pale',\n",
       "  'Sherry',\n",
       "  'would',\n",
       "  'be',\n",
       "  'too',\n",
       "  'rosy',\n",
       "  '-',\n",
       "  'strong',\n",
       "  ';',\n",
       "  'but',\n",
       "  'with',\n",
       "  'whom',\n",
       "  'one',\n",
       "  'sometimes',\n",
       "  'loves',\n",
       "  'to',\n",
       "  'sit',\n",
       "  ',',\n",
       "  'and',\n",
       "  'feel',\n",
       "  'poor',\n",
       "  '-',\n",
       "  'devilish',\n",
       "  ',',\n",
       "  'too',\n",
       "  ';',\n",
       "  'and',\n",
       "  'grow',\n",
       "  'convivial',\n",
       "  'upon',\n",
       "  'tears',\n",
       "  ';',\n",
       "  'and',\n",
       "  'say',\n",
       "  'to',\n",
       "  'them',\n",
       "  'bluntly',\n",
       "  ',',\n",
       "  'with',\n",
       "  'full',\n",
       "  'eyes',\n",
       "  'and',\n",
       "  'empty',\n",
       "  'glasses',\n",
       "  ',',\n",
       "  'and',\n",
       "  'in',\n",
       "  'not',\n",
       "  'altogether',\n",
       "  'unpleasant',\n",
       "  'sadness',\n",
       "  '--',\n",
       "  'Give',\n",
       "  'it',\n",
       "  'up',\n",
       "  ',',\n",
       "  'Sub',\n",
       "  '-',\n",
       "  'Subs',\n",
       "  '!'],\n",
       " ['For',\n",
       "  'by',\n",
       "  'how',\n",
       "  'much',\n",
       "  'the',\n",
       "  'more',\n",
       "  'pains',\n",
       "  'ye',\n",
       "  'take',\n",
       "  'to',\n",
       "  'please',\n",
       "  'the',\n",
       "  'world',\n",
       "  ',',\n",
       "  'by',\n",
       "  'so',\n",
       "  'much',\n",
       "  'the',\n",
       "  'more',\n",
       "  'shall',\n",
       "  'ye',\n",
       "  'for',\n",
       "  'ever',\n",
       "  'go',\n",
       "  'thankless',\n",
       "  '!'],\n",
       " ['Would',\n",
       "  'that',\n",
       "  'I',\n",
       "  'could',\n",
       "  'clear',\n",
       "  'out',\n",
       "  'Hampton',\n",
       "  'Court',\n",
       "  'and',\n",
       "  'the',\n",
       "  'Tuileries',\n",
       "  'for',\n",
       "  'ye',\n",
       "  '!'],\n",
       " ['But',\n",
       "  'gulp',\n",
       "  'down',\n",
       "  'your',\n",
       "  'tears',\n",
       "  'and',\n",
       "  'hie',\n",
       "  'aloft',\n",
       "  'to',\n",
       "  'the',\n",
       "  'royal',\n",
       "  '-',\n",
       "  'mast',\n",
       "  'with',\n",
       "  'your',\n",
       "  'hearts',\n",
       "  ';',\n",
       "  'for',\n",
       "  'your',\n",
       "  'friends',\n",
       "  'who',\n",
       "  'have',\n",
       "  'gone',\n",
       "  'before',\n",
       "  'are',\n",
       "  'clearing',\n",
       "  'out',\n",
       "  'the',\n",
       "  'seven',\n",
       "  '-',\n",
       "  'storied',\n",
       "  'heavens',\n",
       "  ',',\n",
       "  'and',\n",
       "  'making',\n",
       "  'refugees',\n",
       "  'of',\n",
       "  'long',\n",
       "  '-',\n",
       "  'pampered',\n",
       "  'Gabriel',\n",
       "  ',',\n",
       "  'Michael',\n",
       "  ',',\n",
       "  'and',\n",
       "  'Raphael',\n",
       "  ',',\n",
       "  'against',\n",
       "  'your',\n",
       "  'coming',\n",
       "  '.'],\n",
       " ['Here',\n",
       "  'ye',\n",
       "  'strike',\n",
       "  'but',\n",
       "  'splintered',\n",
       "  'hearts',\n",
       "  'together',\n",
       "  '--',\n",
       "  'there',\n",
       "  ',',\n",
       "  'ye',\n",
       "  'shall',\n",
       "  'strike',\n",
       "  'unsplinterable',\n",
       "  'glasses',\n",
       "  '!'],\n",
       " ['EXTRACTS', '.'],\n",
       " ['\"', 'And', 'God', 'created', 'great', 'whales', '.\"'],\n",
       " ['--', 'GENESIS', '.'],\n",
       " ['\"',\n",
       "  'Leviathan',\n",
       "  'maketh',\n",
       "  'a',\n",
       "  'path',\n",
       "  'to',\n",
       "  'shine',\n",
       "  'after',\n",
       "  'him',\n",
       "  ';',\n",
       "  'One',\n",
       "  'would',\n",
       "  'think',\n",
       "  'the',\n",
       "  'deep',\n",
       "  'to',\n",
       "  'be',\n",
       "  'hoary',\n",
       "  '.\"'],\n",
       " ['--', 'JOB', '.'],\n",
       " ['\"',\n",
       "  'Now',\n",
       "  'the',\n",
       "  'Lord',\n",
       "  'had',\n",
       "  'prepared',\n",
       "  'a',\n",
       "  'great',\n",
       "  'fish',\n",
       "  'to',\n",
       "  'swallow',\n",
       "  'up',\n",
       "  'Jonah',\n",
       "  '.\"'],\n",
       " ['--', 'JONAH', '.'],\n",
       " ['\"',\n",
       "  'There',\n",
       "  'go',\n",
       "  'the',\n",
       "  'ships',\n",
       "  ';',\n",
       "  'there',\n",
       "  'is',\n",
       "  'that',\n",
       "  'Leviathan',\n",
       "  'whom',\n",
       "  'thou',\n",
       "  'hast',\n",
       "  'made',\n",
       "  'to',\n",
       "  'play',\n",
       "  'therein',\n",
       "  '.\"'],\n",
       " ['--', 'PSALMS', '.'],\n",
       " ['\"',\n",
       "  'In',\n",
       "  'that',\n",
       "  'day',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Lord',\n",
       "  'with',\n",
       "  'his',\n",
       "  'sore',\n",
       "  ',',\n",
       "  'and',\n",
       "  'great',\n",
       "  ',',\n",
       "  'and',\n",
       "  'strong',\n",
       "  'sword',\n",
       "  ',',\n",
       "  'shall',\n",
       "  'punish',\n",
       "  'Leviathan',\n",
       "  'the',\n",
       "  'piercing',\n",
       "  'serpent',\n",
       "  ',',\n",
       "  'even',\n",
       "  'Leviathan',\n",
       "  'that',\n",
       "  'crooked',\n",
       "  'serpent',\n",
       "  ';',\n",
       "  'and',\n",
       "  'he',\n",
       "  'shall',\n",
       "  'slay',\n",
       "  'the',\n",
       "  'dragon',\n",
       "  'that',\n",
       "  'is',\n",
       "  'in',\n",
       "  'the',\n",
       "  'sea',\n",
       "  '.\"'],\n",
       " ['--', 'ISAIAH'],\n",
       " ['\"',\n",
       "  'And',\n",
       "  'what',\n",
       "  'thing',\n",
       "  'soever',\n",
       "  'besides',\n",
       "  'cometh',\n",
       "  'within',\n",
       "  'the',\n",
       "  'chaos',\n",
       "  'of',\n",
       "  'this',\n",
       "  'monster',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'mouth',\n",
       "  ',',\n",
       "  'be',\n",
       "  'it',\n",
       "  'beast',\n",
       "  ',',\n",
       "  'boat',\n",
       "  ',',\n",
       "  'or',\n",
       "  'stone',\n",
       "  ',',\n",
       "  'down',\n",
       "  'it',\n",
       "  'goes',\n",
       "  'all',\n",
       "  'incontinently',\n",
       "  'that',\n",
       "  'foul',\n",
       "  'great',\n",
       "  'swallow',\n",
       "  'of',\n",
       "  'his',\n",
       "  ',',\n",
       "  'and',\n",
       "  'perisheth',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bottomless',\n",
       "  'gulf',\n",
       "  'of',\n",
       "  'his',\n",
       "  'paunch',\n",
       "  '.\"'],\n",
       " ['--', 'HOLLAND', \"'\", 'S', 'PLUTARCH', \"'\", 'S', 'MORALS', '.'],\n",
       " ['\"',\n",
       "  'The',\n",
       "  'Indian',\n",
       "  'Sea',\n",
       "  'breedeth',\n",
       "  'the',\n",
       "  'most',\n",
       "  'and',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'fishes',\n",
       "  'that',\n",
       "  'are',\n",
       "  ':',\n",
       "  'among',\n",
       "  'which',\n",
       "  'the',\n",
       "  'Whales',\n",
       "  'and',\n",
       "  'Whirlpooles',\n",
       "  'called',\n",
       "  'Balaene',\n",
       "  ',',\n",
       "  'take',\n",
       "  'up',\n",
       "  'as',\n",
       "  'much',\n",
       "  'in',\n",
       "  'length',\n",
       "  'as',\n",
       "  'four',\n",
       "  'acres',\n",
       "  'or',\n",
       "  'arpens',\n",
       "  'of',\n",
       "  'land',\n",
       "  '.\"'],\n",
       " ['--', 'HOLLAND', \"'\", 'S', 'PLINY', '.'],\n",
       " ['\"',\n",
       "  'Scarcely',\n",
       "  'had',\n",
       "  'we',\n",
       "  'proceeded',\n",
       "  'two',\n",
       "  'days',\n",
       "  'on',\n",
       "  'the',\n",
       "  'sea',\n",
       "  ',',\n",
       "  'when',\n",
       "  'about',\n",
       "  'sunrise',\n",
       "  'a',\n",
       "  'great',\n",
       "  'many',\n",
       "  'Whales',\n",
       "  'and',\n",
       "  'other',\n",
       "  'monsters',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sea',\n",
       "  ',',\n",
       "  'appeared',\n",
       "  '.'],\n",
       " ['Among',\n",
       "  'the',\n",
       "  'former',\n",
       "  ',',\n",
       "  'one',\n",
       "  'was',\n",
       "  'of',\n",
       "  'a',\n",
       "  'most',\n",
       "  'monstrous',\n",
       "  'size',\n",
       "  '.'],\n",
       " ['...'],\n",
       " ['This',\n",
       "  'came',\n",
       "  'towards',\n",
       "  'us',\n",
       "  ',',\n",
       "  'open',\n",
       "  '-',\n",
       "  'mouthed',\n",
       "  ',',\n",
       "  'raising',\n",
       "  'the',\n",
       "  'waves',\n",
       "  'on',\n",
       "  'all',\n",
       "  'sides',\n",
       "  ',',\n",
       "  'and',\n",
       "  'beating',\n",
       "  'the',\n",
       "  'sea',\n",
       "  'before',\n",
       "  'him',\n",
       "  'into',\n",
       "  'a',\n",
       "  'foam',\n",
       "  '.\"'],\n",
       " ['--', 'TOOKE', \"'\", 'S', 'LUCIAN', '.'],\n",
       " ['\"', 'THE', 'TRUE', 'HISTORY', '.\"'],\n",
       " ['\"',\n",
       "  'He',\n",
       "  'visited',\n",
       "  'this',\n",
       "  'country',\n",
       "  'also',\n",
       "  'with',\n",
       "  'a',\n",
       "  'view',\n",
       "  'of',\n",
       "  'catching',\n",
       "  'horse',\n",
       "  '-',\n",
       "  'whales',\n",
       "  ',',\n",
       "  'which',\n",
       "  'had',\n",
       "  'bones',\n",
       "  'of',\n",
       "  'very',\n",
       "  'great',\n",
       "  'value',\n",
       "  'for',\n",
       "  'their',\n",
       "  'teeth',\n",
       "  ',',\n",
       "  'of',\n",
       "  'which',\n",
       "  'he',\n",
       "  'brought',\n",
       "  'some',\n",
       "  'to',\n",
       "  'the',\n",
       "  'king',\n",
       "  '.'],\n",
       " ['...'],\n",
       " ['The',\n",
       "  'best',\n",
       "  'whales',\n",
       "  'were',\n",
       "  'catched',\n",
       "  'in',\n",
       "  'his',\n",
       "  'own',\n",
       "  'country',\n",
       "  ',',\n",
       "  'of',\n",
       "  'which',\n",
       "  'some',\n",
       "  'were',\n",
       "  'forty',\n",
       "  '-',\n",
       "  'eight',\n",
       "  ',',\n",
       "  'some',\n",
       "  'fifty',\n",
       "  'yards',\n",
       "  'long',\n",
       "  '.'],\n",
       " ['He',\n",
       "  'said',\n",
       "  'that',\n",
       "  'he',\n",
       "  'was',\n",
       "  'one',\n",
       "  'of',\n",
       "  'six',\n",
       "  'who',\n",
       "  'had',\n",
       "  'killed',\n",
       "  'sixty',\n",
       "  'in',\n",
       "  'two',\n",
       "  'days',\n",
       "  '.\"'],\n",
       " ['--',\n",
       "  'OTHER',\n",
       "  'OR',\n",
       "  'OCTHER',\n",
       "  \"'\",\n",
       "  'S',\n",
       "  'VERBAL',\n",
       "  'NARRATIVE',\n",
       "  'TAKEN',\n",
       "  'DOWN',\n",
       "  'FROM',\n",
       "  'HIS',\n",
       "  'MOUTH',\n",
       "  'BY',\n",
       "  'KING',\n",
       "  'ALFRED',\n",
       "  ',',\n",
       "  'A',\n",
       "  '.',\n",
       "  'D',\n",
       "  '.',\n",
       "  '890',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'And',\n",
       "  'whereas',\n",
       "  'all',\n",
       "  'the',\n",
       "  'other',\n",
       "  'things',\n",
       "  ',',\n",
       "  'whether',\n",
       "  'beast',\n",
       "  'or',\n",
       "  'vessel',\n",
       "  ',',\n",
       "  'that',\n",
       "  'enter',\n",
       "  'into',\n",
       "  'the',\n",
       "  'dreadful',\n",
       "  'gulf',\n",
       "  'of',\n",
       "  'this',\n",
       "  'monster',\n",
       "  \"'\",\n",
       "  's',\n",
       "  '(',\n",
       "  'whale',\n",
       "  \"'\",\n",
       "  's',\n",
       "  ')',\n",
       "  'mouth',\n",
       "  ',',\n",
       "  'are',\n",
       "  'immediately',\n",
       "  'lost',\n",
       "  'and',\n",
       "  'swallowed',\n",
       "  'up',\n",
       "  ',',\n",
       "  'the',\n",
       "  'sea',\n",
       "  '-',\n",
       "  'gudgeon',\n",
       "  'retires',\n",
       "  'into',\n",
       "  'it',\n",
       "  'in',\n",
       "  'great',\n",
       "  'security',\n",
       "  ',',\n",
       "  'and',\n",
       "  'there',\n",
       "  'sleeps',\n",
       "  '.\"'],\n",
       " ['--', 'MONTAIGNE', '.'],\n",
       " ['--', 'APOLOGY', 'FOR', 'RAIMOND', 'SEBOND', '.'],\n",
       " ['\"', 'Let', 'us', 'fly', ',', 'let', 'us', 'fly', '!'],\n",
       " ['Old',\n",
       "  'Nick',\n",
       "  'take',\n",
       "  'me',\n",
       "  'if',\n",
       "  'is',\n",
       "  'not',\n",
       "  'Leviathan',\n",
       "  'described',\n",
       "  'by',\n",
       "  'the',\n",
       "  'noble',\n",
       "  'prophet',\n",
       "  'Moses',\n",
       "  'in',\n",
       "  'the',\n",
       "  'life',\n",
       "  'of',\n",
       "  'patient',\n",
       "  'Job',\n",
       "  '.\"'],\n",
       " ['--', 'RABELAIS', '.'],\n",
       " ['\"', 'This', 'whale', \"'\", 's', 'liver', 'was', 'two', 'cartloads', '.\"'],\n",
       " ['--', 'STOWE', \"'\", 'S', 'ANNALS', '.'],\n",
       " ['\"',\n",
       "  'The',\n",
       "  'great',\n",
       "  'Leviathan',\n",
       "  'that',\n",
       "  'maketh',\n",
       "  'the',\n",
       "  'seas',\n",
       "  'to',\n",
       "  'seethe',\n",
       "  'like',\n",
       "  'boiling',\n",
       "  'pan',\n",
       "  '.\"'],\n",
       " ['--', 'LORD', 'BACON', \"'\", 'S', 'VERSION', 'OF', 'THE', 'PSALMS', '.'],\n",
       " ['\"',\n",
       "  'Touching',\n",
       "  'that',\n",
       "  'monstrous',\n",
       "  'bulk',\n",
       "  'of',\n",
       "  'the',\n",
       "  'whale',\n",
       "  'or',\n",
       "  'ork',\n",
       "  'we',\n",
       "  'have',\n",
       "  'received',\n",
       "  'nothing',\n",
       "  'certain',\n",
       "  '.'],\n",
       " ['They',\n",
       "  'grow',\n",
       "  'exceeding',\n",
       "  'fat',\n",
       "  ',',\n",
       "  'insomuch',\n",
       "  'that',\n",
       "  'an',\n",
       "  'incredible',\n",
       "  'quantity',\n",
       "  'of',\n",
       "  'oil',\n",
       "  'will',\n",
       "  'be',\n",
       "  'extracted',\n",
       "  'out',\n",
       "  'of',\n",
       "  'one',\n",
       "  'whale',\n",
       "  '.\"'],\n",
       " ['--', 'IBID', '.'],\n",
       " ['\"', 'HISTORY', 'OF', 'LIFE', 'AND', 'DEATH', '.\"'],\n",
       " ['\"',\n",
       "  'The',\n",
       "  'sovereignest',\n",
       "  'thing',\n",
       "  'on',\n",
       "  'earth',\n",
       "  'is',\n",
       "  'parmacetti',\n",
       "  'for',\n",
       "  'an',\n",
       "  'inward',\n",
       "  'bruise',\n",
       "  '.\"'],\n",
       " ['--', 'KING', 'HENRY', '.'],\n",
       " ['\"', 'Very', 'like', 'a', 'whale', '.\"'],\n",
       " ['--', 'HAMLET', '.'],\n",
       " ['\"',\n",
       "  'Which',\n",
       "  'to',\n",
       "  'secure',\n",
       "  ',',\n",
       "  'no',\n",
       "  'skill',\n",
       "  'of',\n",
       "  'leach',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'art',\n",
       "  'Mote',\n",
       "  'him',\n",
       "  'availle',\n",
       "  ',',\n",
       "  'but',\n",
       "  'to',\n",
       "  'returne',\n",
       "  'againe',\n",
       "  'To',\n",
       "  'his',\n",
       "  'wound',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'worker',\n",
       "  ',',\n",
       "  'that',\n",
       "  'with',\n",
       "  'lowly',\n",
       "  'dart',\n",
       "  ',',\n",
       "  'Dinting',\n",
       "  'his',\n",
       "  'breast',\n",
       "  ',',\n",
       "  'had',\n",
       "  'bred',\n",
       "  'his',\n",
       "  'restless',\n",
       "  'paine',\n",
       "  ',',\n",
       "  'Like',\n",
       "  'as',\n",
       "  'the',\n",
       "  'wounded',\n",
       "  'whale',\n",
       "  'to',\n",
       "  'shore',\n",
       "  'flies',\n",
       "  'thro',\n",
       "  \"'\",\n",
       "  'the',\n",
       "  'maine',\n",
       "  '.\"'],\n",
       " ['--', 'THE', 'FAERIE', 'QUEEN', '.'],\n",
       " ['\"',\n",
       "  'Immense',\n",
       "  'as',\n",
       "  'whales',\n",
       "  ',',\n",
       "  'the',\n",
       "  'motion',\n",
       "  'of',\n",
       "  'whose',\n",
       "  'vast',\n",
       "  'bodies',\n",
       "  'can',\n",
       "  'in',\n",
       "  'a',\n",
       "  'peaceful',\n",
       "  'calm',\n",
       "  'trouble',\n",
       "  'the',\n",
       "  'ocean',\n",
       "  'til',\n",
       "  'it',\n",
       "  'boil',\n",
       "  '.\"'],\n",
       " ['--', 'SIR', 'WILLIAM', 'DAVENANT', '.'],\n",
       " ['PREFACE', 'TO', 'GONDIBERT', '.'],\n",
       " ['\"',\n",
       "  'What',\n",
       "  'spermacetti',\n",
       "  'is',\n",
       "  ',',\n",
       "  'men',\n",
       "  'might',\n",
       "  'justly',\n",
       "  'doubt',\n",
       "  ',',\n",
       "  'since',\n",
       "  'the',\n",
       "  'learned',\n",
       "  'Hosmannus',\n",
       "  'in',\n",
       "  'his',\n",
       "  'work',\n",
       "  'of',\n",
       "  'thirty',\n",
       "  'years',\n",
       "  ',',\n",
       "  'saith',\n",
       "  'plainly',\n",
       "  ',',\n",
       "  'Nescio',\n",
       "  'quid',\n",
       "  'sit',\n",
       "  '.\"'],\n",
       " ['--', 'SIR', 'T', '.', 'BROWNE', '.'],\n",
       " ['OF', 'SPERMA', 'CETI', 'AND', 'THE', 'SPERMA', 'CETI', 'WHALE', '.'],\n",
       " ['VIDE', 'HIS', 'V', '.', 'E', '.'],\n",
       " ['\"',\n",
       "  'Like',\n",
       "  'Spencer',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'Talus',\n",
       "  'with',\n",
       "  'his',\n",
       "  'modern',\n",
       "  'flail',\n",
       "  'He',\n",
       "  'threatens',\n",
       "  'ruin',\n",
       "  'with',\n",
       "  'his',\n",
       "  'ponderous',\n",
       "  'tail',\n",
       "  '.'],\n",
       " ['...',\n",
       "  'Their',\n",
       "  'fixed',\n",
       "  'jav',\n",
       "  \"'\",\n",
       "  'lins',\n",
       "  'in',\n",
       "  'his',\n",
       "  'side',\n",
       "  'he',\n",
       "  'wears',\n",
       "  ',',\n",
       "  'And',\n",
       "  'on',\n",
       "  'his',\n",
       "  'back',\n",
       "  'a',\n",
       "  'grove',\n",
       "  'of',\n",
       "  'pikes',\n",
       "  'appears',\n",
       "  '.\"'],\n",
       " ['--', 'WALLER', \"'\", 'S', 'BATTLE', 'OF', 'THE', 'SUMMER', 'ISLANDS', '.'],\n",
       " ['\"',\n",
       "  'By',\n",
       "  'art',\n",
       "  'is',\n",
       "  'created',\n",
       "  'that',\n",
       "  'great',\n",
       "  'Leviathan',\n",
       "  ',',\n",
       "  'called',\n",
       "  'a',\n",
       "  'Commonwealth',\n",
       "  'or',\n",
       "  'State',\n",
       "  '--(',\n",
       "  'in',\n",
       "  'Latin',\n",
       "  ',',\n",
       "  'Civitas',\n",
       "  ')',\n",
       "  'which',\n",
       "  'is',\n",
       "  'but',\n",
       "  'an',\n",
       "  'artificial',\n",
       "  'man',\n",
       "  '.\"']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] # sampling sentences for test\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus] #for lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:35:53.500102Z",
     "start_time": "2018-09-21T19:35:53.496114Z"
    }
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus)) # count each word\n",
    "border = int(len(word_count) * 0.01) # border of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:39:57.857307Z",
     "start_time": "2018-09-21T19:39:57.850329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 1),\n",
       " ('artificial', 1),\n",
       " ('Civitas', 1),\n",
       " ('Latin', 1),\n",
       " ('--(', 1),\n",
       " ('State', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:36:03.767082Z",
     "start_time": "2018-09-21T19:36:03.763095Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border] + list(reversed(word_count.most_common()))[:border] # most common words and most rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:36:10.851808Z",
     "start_time": "2018-09-21T19:36:10.848817Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = [s[0] for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:36:28.275333Z",
     "start_time": "2018-09-21T19:36:28.271344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'the',\n",
       " 'of',\n",
       " 'and',\n",
       " '--',\n",
       " 'man',\n",
       " 'artificial',\n",
       " 'Civitas',\n",
       " 'Latin',\n",
       " '--(',\n",
       " 'State']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:40:40.635878Z",
     "start_time": "2018-09-21T19:40:40.632886Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
    "vocab.append('<UNK>') #corpus에는 있는데 vocab에는 없으니까 <unknown으로>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:41:17.896532Z",
     "start_time": "2018-09-21T19:41:17.893511Z"
    }
   },
   "outputs": [],
   "source": [
    "word2index = {'<UNK>' : 0} \n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:44:16.769583Z",
     "start_time": "2018-09-21T19:44:16.764597Z"
    }
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "# how can we interpret this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:52:30.027309Z",
     "start_time": "2018-09-21T19:52:30.016339Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'Moby'), ('[', 'Dick'), ('[', 'by'), ('Moby', '['), ('Moby', 'Dick'), ('Moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "\n",
    "print(train_data[:WINDOW_SIZE * 2])\n",
    "# extract all context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:57:09.863887Z",
     "start_time": "2018-09-21T19:57:09.859902Z"
    }
   },
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:57:41.624036Z",
     "start_time": "2018-09-21T19:57:41.517818Z"
    }
   },
   "outputs": [],
   "source": [
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:58:00.572437Z",
     "start_time": "2018-09-21T19:58:00.568448Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:58:05.794435Z",
     "start_time": "2018-09-21T19:58:05.789449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:58:13.386439Z",
     "start_time": "2018-09-21T19:58:13.380455Z"
    }
   },
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # init\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # init\n",
    "        #self.out = nn.Linear(projection_dim,vocab_size)\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D, B는 batch size\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D, 주변 단어 중 하나\n",
    "        outer_embeds = self.embedding_u(outer_words) # B x V x D, 전체 단어\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1xD * BxDx1 => Bx1 , bmm은 Batch는 그대로 두고 곱해라\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # BxVxD * BxDx1 => BxV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax, unsqueeze는 X1을 뒤에다가 붙이는 것(차원 높이는)\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:58:19.270803Z",
     "start_time": "2018-09-21T19:58:19.266814Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T19:58:24.188450Z",
     "start_time": "2018-09-21T19:58:24.183465Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T20:11:10.850275Z",
     "start_time": "2018-09-21T20:00:10.678282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.26\n",
      "Epoch : 10, mean_loss : 4.38\n",
      "Epoch : 20, mean_loss : 3.41\n",
      "Epoch : 30, mean_loss : 3.25\n",
      "Epoch : 40, mean_loss : 3.20\n",
      "Epoch : 50, mean_loss : 3.18\n",
      "Epoch : 60, mean_loss : 3.17\n",
      "Epoch : 70, mean_loss : 3.16\n",
      "Epoch : 80, mean_loss : 3.15\n",
      "Epoch : 90, mean_loss : 3.15\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs)\n",
    "        targets = torch.cat(targets)\n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))  # B x V\n",
    "        \n",
    "        model.zero_grad() # 여기부터 4줄은 자주 쓰니 기억할 것\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "        losses.append(loss.data.tolist[0])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T20:12:00.244044Z",
     "start_time": "2018-09-21T20:12:00.235102Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    if USE_CUDA:\n",
    "        target_V = model.prediction(prepare_word(target, word2index)) #useless\n",
    "    else:\n",
    "        target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: continue\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        else:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] # .item으로 하면 된다, word vector를 뽑아낼 수 있다\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10] # sort by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T20:12:01.036608Z",
     "start_time": "2018-09-21T20:12:01.031619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poets'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T20:12:01.858152Z",
     "start_time": "2018-09-21T20:12:01.802302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['here', 0.7442889213562012],\n",
       " ['generally', 0.6463077068328857],\n",
       " ['appearing', 0.6241486668586731],\n",
       " ['these', 0.5828226208686829],\n",
       " ['as', 0.5827580094337463],\n",
       " ['well', 0.5705728530883789],\n",
       " ['ISAIAH', 0.5673191547393799],\n",
       " ['whether', 0.544781506061554],\n",
       " ['however', 0.5162895321846008],\n",
       " ['devil', 0.5149803161621094]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
