{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- random variable의 parameter를 estimate하는 방법 중 하나입니다.\n",
    "- 오직 주어진 Observation, 혹은 데이터들 만을 토대로 parameter estimation을 하는 방법입니다. \n",
    "- 우리는 현재 우리가 갖고 있는 데이터가 가장 나옴직한 parameter를 고르고 싶을 것이고, 바로 이게 maximum likelihood estimate 입니다.<br><br>\n",
    "\n",
    "$$ \\underset{\\theta}{\\arg\\max}\\ P(y\\,\\vert\\, x; \\theta) $$\n",
    "\n",
    "다만 확률 값은 [0,1] 안에서 결정되기 때문에 확률 값들을 여러 차례 곱하면 값이 매우 빠르게 작아집니다.<br>\n",
    "이런 현상을 방지하기 위해 log-likelihood를 사용하게 되는 것입니다. 또한 log의 성질 덕에 곱하기 연산이 더하기로 바뀌게 됩니다.\n",
    "\n",
    "보통 negative log likelihood(NLL) function 을 **목적함수**로 놓고 최소화 시키면서 최적의 parameter 를 찾는 것이 딥러닝 학습의 시작단계라고 할 수 있습니다.\n",
    "\n",
    "$$ \\underset{\\theta}{\\arg\\min}\\ -logP(y\\,\\vert\\, x; \\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화 문제는 함수 $f$의 값을 최대화 혹은 최소화하는 변수 $x$의 값 $x^{\\ast}$를 찾는 것입니다. 수식으로는 다음처럼 씁니다.\n",
    "\n",
    "$$ x^{\\ast} = \\arg \\max_x f(x) \\;\\;(\\text{최대화의 경우}) $$\n",
    "\n",
    "$$ x^{\\ast} = \\arg \\min_x f(x) \\;\\;(\\text{최소화의 경우}) $$\n",
    "\n",
    "이 값 $x^{\\ast}$를 최적화 문제의 해(solution)라고 합니다.\n",
    "\n",
    "최대화 문제는 $f(x)$ 를 $-f(x)$ 로 바꾸면 풀 수 있으므로 보통 최소화의 경우만 고려합니다.\n",
    "\n",
    "이 때 최소화하고자 하는 함수 $f(x)$를 **목적함수(objective function)**, **비용함수(cost function)** 또는 **손실함수(loss function)** 등으로 부릅니다. 기호로는 각각 $J, C, L$로 표기하는 경우가 많습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight(parameter) Update\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1y7KfiZ82K7A5U0Ug2T7eyqVblkKWFTsF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "### 1. Mean Squared Error - Regression\n",
    "\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-9b581a798d41c0cfec39bac20ee6fb50_l3.png\">\n",
    "\n",
    "이를 Neural Networks의 모델에 적용해서 표현하면 Neural Networks에서 output unit에서의 Cost function-J(W,b;x,y)-은 모델의 예측값(Prediction)-$\\sigma(z)$-과 실제 타겟값(True target value)-$y$-과의 차이를 제곱한 값으로 정의됩니다. (참고 : $z=Wx+b$, 계산의 편의성을 위해서 $\\frac{1}{2}$을 식에 추가함)\n",
    "\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-a39c609db4ee22c8d2c903662bb7eb0b_l3.png\">\n",
    "\n",
    "Neural Networks를 학습시키기 위해서는 Cost function-J(W,b;x,y)-의 미분값(derivative)를 계산해서 Neural Networks의 파라미터 W, b를 업데이트해야합니다.MSE Cost function을 이용했을 경우 Weight와 Bias의 미분값(Derivative)를 계산하면 아래와 같습니다.\n",
    "(참고 : $\\sigma$ 는 activation function )\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1FhQqiE5zevi_1CWvTSK4VKdj8tzUkztN\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Average Cross Entropy Loss - Classification\n",
    "\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-49fd6c934151da1c89e8caf78cff9a82_l3.png\">\n",
    "\n",
    "Cross-Entropy Cost Function을 이용했을 때 Neural Networks의 파라미터인 Weight의 미분값(Derivative)를 계산하면 아래와 같습니다.<br><br>\n",
    "\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-c7343077fef6b09b1dbdc5238dcfa349_l3.png\"><br>\n",
    "<img src=\"https://drive.google.com/uc?id=14-jR7eoynt5RajadL-l8MThfVGZRJEZH\"><br>\n",
    "sigmoid function은 $\\sigma(z)=\\frac{1}{(1+e^{-z})}$로 정의되고 이를 미분하면 $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$입니다. <br>\n",
    "따라서 이를 이용해 위 식을 다시한번 정리하면 최종적으로 Cross-Entropy를 이용한 Cost Function-J(W,b;x,y)-의 Weight와 bias의 미분값(Derivative)은 <br>\n",
    "아래와 같이 계산됩니다.\n",
    "\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-200646d09711afeb4bdde2504dd81d74_l3.png\"><br>\n",
    "<img src=\"http://solarisailab.com/wp-content/ql-cache/quicklatex.com-a91b5960b3fee32a6d282dc3e453ee9f_l3.png\"><br>\n",
    "\n",
    "위 식에서 파라미터 Weight와 Bias의 미분값(Derivative)은 예측값과 실제값의 차이$(\\sigma(z)-y)$에 비례합니다. <br>\n",
    "따라서 오차가 더 큰 인풋값에 대해서는 더 많이 업데이트하고, 오차가 더 작은 인풋값에 대해서는 더 적게 업데이트하는 결과를 얻을 수 있습니다.\n",
    "\n",
    "또한, 미분값에 $\\sigma'(z)$가 포함되어 있지 않아 sigmoid function의 특성때문에 발생하는 학습저하(slowdown) 문제도 발생하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1wOCO2RDqgIrwfEPRRbRJ-MEGmQ36Cmgs\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[참고 Link]\n",
    "- <a href=\"https://datascienceschool.net/view-notebook/4642b9f187784444b8f3a8309c583007\"> 데이터 사이언스 스쿨 - 최적화 기초</a>\n",
    "\n",
    "- <a href=\"http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html\"> BEOMSU KIM'S BLOG - Gradient Descent Optimization Algorithms 정리</a>\n",
    "\n",
    "- <a href=\"http://solarisailab.com/archives/2237\"> 솔라리스의 인공지능 연구실 - Neural Networks의 학습속도 저하(Slowdown)를 막는 Cross-Entropy Cost Function</a>\n",
    "\n",
    "- <a href=\"http://sanghyukchun.github.io/58/\"> SanghyukChun's Blog - Machine Learning 스터디 (2) Probability Theory</a>\n",
    "\n",
    "- <a href=\"http://jaejunyoo.blogspot.com/2018/02/minimizing-negative-log-likelihood-in-kor-3.html\"> Jaejun Yoo's Playground - Minimizing the Negative Log-Likelihood, in Korean (3)</a>\n",
    "\n",
    "- <a href=\"https://taeoh-kim.github.io/blog/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8/\"> Taeoh Kim's Blog - Cross Entropy의 정확한 확률적 의미</a>\n",
    "\n",
    "- <a href=\"http://funmv2013.blogspot.com/2017/01/cross-entropy.html\"> funMV - 분류 오차에 Cross entropy를 사용하는 이유</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
